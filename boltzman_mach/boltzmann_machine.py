# -*- coding: utf-8 -*-
"""Boltzmann Machine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JxIEe_TAcz-utfLKTqjbo3foOtqDGD0s

#Boltzmann Machine

##Downloading the dataset

###ML-100K
"""

'''
!wget "http://files.grouplens.org/datasets/movielens/ml-100k.zip"
!unzip ml-100k.zip
!ls
'''

"""###ML-1M"""

'''
!wget "http://files.grouplens.org/datasets/movielens/ml-1m.zip"
!unzip ml-1m.zip
!ls
'''

"""##Importing the libraries"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable
import os

# Importing the dataset 
'''
We won't be using this dataset.

movies = pd.read_csv(
    'ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1'
)
users = pd.read_csv(
    'ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1'
)
ratings = pd.read_csv(
    'ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1'
)
'''

# path / file of train & test set
string_path_base = os.path.abspath('.')
# string_path_boltz = os.path.join(string_path_base, 'boltzman_mach')
string_path_boltz = string_path_base
string_pf_train = os.path.join(string_path_boltz, 'ml-100k/u1.base')
string_pf_test = os.path.join(string_path_boltz, 'ml-100k/u1.test')

# Preparing the training set and the test set
training_set = pd.read_csv(string_pf_train, delimiter = '\t')
training_set = np.array(training_set, dtype = 'int')
test_set = pd.read_csv(string_pf_test, delimiter = '\t')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))

def convert(data):
    '''
    Converting the data into an array with users in lines and movies in columns

    :param numpy.array data: data to convert
        array[0] -> int; user id
        array[1] -> int; movie id
        array[2] -> float; movie rating; 1 to 5
        array[3] -> int; timestamp
    :rtype: list of lists
    :return: data converted into the proper format
        row -> ratings of all movies by one user
        columns -> movies to be rated
    '''
    new_data = list()
    for id_users in range(1, nb_users + 1):
        id_movies = data[:, 1][data[:, 0] == id_users]
        id_ratings = data[:, 2][data[:, 0] == id_users]
        ratings = np.zeros(nb_movies)
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data
training_set = convert(training_set)
test_set = convert(test_set)

# Converting the data into Torch tensors (expects list of lists)
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

# Converting the ratings into binary ratings 1 (Liked) or 0 (Not Liked)
training_set[training_set == 0] = -1
training_set[training_set == 1] = 0
training_set[training_set == 2] = 0
training_set[training_set >= 3] = 1
test_set[test_set == 0] = -1
test_set[test_set == 1] = 0
test_set[test_set == 2] = 0
test_set[test_set >= 3] = 1

# Creating the architecture of the Neural Network
class RBM(object):
    '''
    resctricted boltzman machine

    <add comments here>
    '''
    def __init__(self, nv, nh):
        '''
        constructor for class; initialized the weights matrix;
        a -> probility of hidden nodes given visible nodes
        b -> probability of visible nodes given hidden nodes

        :param int nv: number of visible nodes
        :param int nh: number of hidden nodes
        '''
        self.W = torch.randn(nh, nv)
        self.a = torch.randn(1, nh)
        self.b = torch.randn(1, nv)
    
    def sample_h(self, x):
        '''
        sample hidden nodes of the RBM; use sigmoid function to determine if nodes
        are actrivated or not

        :param torch.tensor x: vissible neurons
        :rtype: tuple
        :return: probabilities and activated hidden nodes
            tuple[0] -> torch.tensor; probabilities of activtion of hidden nodes 
            tuple[1] -> torch.tesnor; activated / not activated nodes
        '''
        wx = torch.mm(x, self.W.t())
        activation = wx + self.a.expand_as(wx)
        p_h_given_v = torch.sigmoid(activation)
        return p_h_given_v, torch.bernoulli(p_h_given_v)
    
    def sample_v(self, y):
        '''
        sample visbile nodes of the RBM; use sigmoid function to determine if nodes
        are actrivated or not

        :param torch.tensor y: hidden neurons
        :rtype: tuple
        :return: probabilities and activated visbile nodes
            tuple[0] -> torch.tensor; probabilities of activtion of nodes 
            tuple[1] -> torch.tesnor; activated / not activated nodes
        '''
        wy = torch.mm(y, self.W)
        activation = wy + self.b.expand_as(wy)
        p_v_given_h = torch.sigmoid(activation)
        return p_v_given_h, torch.bernoulli(p_v_given_h)

    def train(self, v0, vk, ph0, phk):
        '''
        training method; implemenatation of contrastive divergence

        :param torch.tensor v0: input vector
        :param torch.tensor vk: visible nodes after k-times sampling
        :param torch.tensor ph0: probabilities of at the first iteration that the values
            equal 1 for v0
        :param torch.tensor phk: probabilities of the hiddens nodes after k-times sampling
            given the values of the nodes in vk
        :rtype: n/a
        :return: None
        '''
        self.W += (torch.mm(v0.t(), ph0) - torch.mm(vk.t(), phk)).t()
        self.b += torch.sum((v0 - vk), 0)
        self.a += torch.sum((ph0 - phk), 0)

# set-up for RBM object
nv = len(training_set[0])
nh = 100 # number of features to detect
batch_size = 100
rbm = RBM(nv, nh)

# Training the RBM
nb_epoch = 10
for epoch in range(1, nb_epoch + 1):
    train_loss = 0
    s = 0.
    for id_user in range(0, nb_users - batch_size, batch_size):
        vk = training_set[id_user:id_user + batch_size]
        v0 = training_set[id_user:id_user + batch_size]
        ph0, _ = rbm.sample_h(v0)
        for k in range(0, 10): # k steps of contrastive divergence (random walk)
            _, hk = rbm.sample_h(vk)
            _, vk = rbm.sample_v(hk)
            vk[v0 < 0] = v0[v0 < 0] # replace -1 rating (not rated by users) w/ 0
        phk, _ = rbm.sample_h(vk)
        rbm.train(v0, vk, ph0, phk)
        
        # loss functions
        train_loss += torch.mean(torch.abs(v0[v0 >= 0] - vk[v0 >= 0])) # mean distance
        # train_loss += np.sqrt(torch.mean((v0[v0 >= 0] - vk[v0 >= 0])**2)) # RMSE
        
        # increment counter
        s += 1.
    print('epoch: ' + str(epoch) + ' loss: ' + str(train_loss/s))

# Testing the RBM
test_loss = 0
s = 0.
for id_user in range(0, nb_users):
    v = training_set[id_user:id_user + 1] # use inputs from training set to activate test set
    vt = test_set[id_user:id_user + 1]
    if len(vt[vt>=0]) > 0:
        _, h = rbm.sample_h(v)
        _, v = rbm.sample_v(h)

        # loss functions
        test_loss += torch.mean(torch.abs(vt[vt >= 0] - v[vt >= 0])) # mean distance
        # test_loss += np.sqrt(torch.mean((vt[vt >= 0] - v[vt >= 0])**2)) # RMSE

        # increment counter
        s += 1.
print('test loss: ' + str(test_loss/s))

# -*- coding: utf-8 -*-
'''
AutoEncoders.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KYKvdVQ_r32eT7o-_QqOcD_XtCc92F1t

AutoEncoders

Downloading the dataset

ML-100K
!wget "http://files.grouplens.org/datasets/movielens/ml-100k.zip"
!unzip ml-100k.zip
!ls

ML-1M
!wget "http://files.grouplens.org/datasets/movielens/ml-1m.zip"
!unzip ml-1m.zip
!ls
'''

#Importing the libraries
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable
import os

# Importing the dataset
'''
# We won't be using this dataset.
movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')
users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')
ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')
'''

# path / file of train & test set
string_path_base = os.path.abspath('.')
# string_path_boltz = os.path.join(string_path_base, 'boltzman_mach')
string_path_boltz = string_path_base
string_pf_train = os.path.join(string_path_boltz, 'ml-100k/u1.base')
string_pf_test = os.path.join(string_path_boltz, 'ml-100k/u1.test')

# Preparing the training set and the test set
training_set = pd.read_csv(string_pf_train, delimiter = '\t')
training_set = np.array(training_set, dtype = 'int')
test_set = pd.read_csv(string_pf_test, delimiter = '\t')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0], ), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1], ), max(test_set[:, 1])))

def convert(data):
    '''
    Converting the data into an array with users in lines and movies in columns

    :param numpy.array data: data to convert
        array[0] -> int; user id
        array[1] -> int; movie id
        array[2] -> float; movie rating; 1 to 5
        array[3] -> int; timestamp
    :rtype: list of lists
    :return: data converted into the proper format
        row -> ratings of all movies by one user
        columns -> movies to be rated
    '''
    new_data = list()
    for id_users in range(1, nb_users + 1):
        id_movies = data[:, 1][data[:, 0] == id_users]
        id_ratings = data[:, 2][data[:, 0] == id_users]
        ratings = np.zeros(nb_movies)
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data
training_set = convert(training_set)
test_set = convert(test_set)

# Converting the data into Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

# Creating the architecture of the Neural Network
class SAE(nn.Module):
    '''
    stacked auto encocder (SAE) class

    <add comment here>
    '''
    def __init__(self, ):
        '''
        constructor for auto encoder; calls super constructor nn.Module
        '''
        super(SAE, self).__init__()
        self.fc1 = nn.Linear(nb_movies, 20)
        self.fc2 = nn.Linear(20, 10)
        self.fc3 = nn.Linear(10, 20)
        self.fc4 = nn.Linear(20, nb_movies)
        self.activation = nn.Sigmoid()

    def forward(self, x):
        '''
        encoding and decodings of the auto encoder

        :param ?? x: input vector
        :rtype: ??
        :return: output vector of predicted ratings
        '''
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        x = self.fc4(x)
        return x

# set-up for auto encoder
sae = SAE()
criterion = nn.MSELoss()
optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)

# Training the SAE
nb_epoch = 200
for epoch in range(1, nb_epoch + 1):
    train_loss = 0
    s = 0.
    for id_user in range(0, nb_users):
        input = Variable(training_set[id_user]).unsqueeze(0)
        target = input.clone()
        if torch.sum(target.data > 0) > 0:
            output = sae(input)
            target.require_grad = False
            output[target == 0] = 0
            loss = criterion(output, target)
            mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)
            loss.backward() # determines direction
            train_loss += np.sqrt(loss.data * mean_corrector)
            s += 1.
            optimizer.step() # determines intensity
    print('epoch: {}, loss: {}'.format(epoch, train_loss / s))

# Testing the SAE
test_loss = 0
s = 0.
for id_user in range(0, nb_users):
    input = Variable(training_set[id_user]).unsqueeze(0)
    target = Variable(test_set[id_user]).unsqueeze(0)
    if torch.sum(target.data > 0) > 0:
        output = sae(input)
        target.require_grad = False
        output[target == 0] = 0
        loss = criterion(output, target)
        mean_corrector = nb_movies / float(torch.sum(target.data > 0) + 1e-10)
        test_loss += np.sqrt(loss.data * mean_corrector)
        s += 1.
print('\ntest loss: {}'.format(test_loss / s))
